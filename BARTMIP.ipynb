{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, spearmanr\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, LeavePGroupsOut, LeaveOneGroupOut, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tabpfn import TabPFNRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "from sklearn_extra.cluster import KMedoids \n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import random\n",
    "seed = random.randint(0, 100000)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c79e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('all-features-imputed-v2.csv')\n",
    "\n",
    "#display(data)\n",
    "#display(data.columns.to_list())\n",
    "\n",
    "\n",
    "acceleration    = data.filter(like='acceleration').columns.tolist()\n",
    "heartrate       = data.filter(like='heartrate').filter(regex='^(?!.*sleep)').columns.tolist()\n",
    "motion          = data.filter(like='motion').columns.tolist()\n",
    "position        = data.filter(like='position').columns.tolist()\n",
    "sleep           = data.filter(like='sleep').columns.tolist()\n",
    "step            = data.filter(like='step').columns.tolist()\n",
    "demographics    = ['age','sex']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ee52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = acceleration + heartrate + motion + position + sleep + step + demographics\n",
    "\n",
    "sensor = data[modalities]\n",
    "\n",
    "ohs = data['ohs']\n",
    "sis = data['sis']\n",
    "\n",
    "participant = data['participant']\n",
    "\n",
    "x = np.array(sensor)\n",
    "#Set target variable\n",
    "y = np.array(ohs)\n",
    "p = np.array(participant)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38574f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TRUES = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "SHAP = []\n",
    "X_TEST = []\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "    \n",
    "    print(f\"Fold {fold}: train={len(train_idx)} test={len(test_idx)}\")\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    x_train = normalizer.fit_transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=3,\n",
    "        loss_function='RMSE',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, y_train, eval_set=(x_train, y_train), use_best_model=True, early_stopping_rounds=100)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "    SHAP.append(shap_values)\n",
    "    X_TEST.append(x_test)\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': modalities,\n",
    "    'mean_abs_shap': np.abs(np.vstack(SHAP)).mean(axis=0).round(4)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "# display(shap_df)\n",
    "#shap.summary_plot(np.vstack(SHAP), pd.DataFrame(np.vstack(X_TEST), columns=modalities), max_display=24)\n",
    "\n",
    "\n",
    "\n",
    "# Select the first num_features features based on SHAP importance + demographics\n",
    "num_features = 16\n",
    "x = np.array(data[\n",
    "    shap_df['feature'].iloc[:num_features].to_list()\n",
    "     + demographics\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce51438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biweekly_scores = y.reshape(-1, 7)[:, 0]  # Shape: (80 weeks, 7 day)\n",
    "weekly_bags = x.reshape((80, 7, -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492c5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_connectivity(distance_matrix, cluster_labels, k_neighbors=5):\n",
    "    \"\"\"\n",
    "    Compute connectivity score for clustering results.\n",
    "    Lower values indicate better connectivity (0 is perfect).\n",
    "    \"\"\"\n",
    "    n_samples = distance_matrix.shape[0]\n",
    "    \n",
    "    # Get k nearest neighbors for each point (excluding self)\n",
    "    neighbor_indices = np.argsort(distance_matrix, axis=1)[:, 1:k_neighbors+1]  # Exclude self\n",
    "    \n",
    "    connectivity = 0\n",
    "    for i in range(n_samples):\n",
    "        for neighbor in neighbor_indices[i]:\n",
    "            if cluster_labels[i] != cluster_labels[neighbor]:\n",
    "                connectivity += 1\n",
    "                \n",
    "    # Normalize to [0,1] range\n",
    "    max_possible = n_samples * k_neighbors\n",
    "    return connectivity / max_possible if max_possible > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "def compute_distance_matrix(bags): #OBS CHECK VALIDITY\n",
    "    n= len(bags)\n",
    "    #print(\"BAGS:\",bags)\n",
    "    matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dist = average_hausdorff_distance(bags[i], bags[j])\n",
    "            matrix[i, j] = matrix[j, i] = dist\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def clustering(bags,matrix, n_clusters=10):#OBS MAYBE KMEANS???\n",
    "    \"\"\"Perform Clustering using K-medoids\"\"\"\n",
    "    kmedoids=KMedoids(n_clusters=n_clusters, metric='precomputed', method='pam', init=\"build\")\n",
    "    #print(kmedoids)\n",
    "                \n",
    "    clusters = kmedoids.fit_predict(matrix)\n",
    "    silhouette_avg = silhouette_score(matrix, clusters, metric=\"precomputed\")\n",
    "    connectivity = compute_connectivity(matrix, clusters)\n",
    "\n",
    "    #print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "    valid_medoids = [idx for idx in kmedoids.medoid_indices_ if idx < len(bags)]\n",
    "    medoids = [bags[idx] for idx in valid_medoids]\n",
    "\n",
    "    return medoids, silhouette_avg , connectivity\n",
    "\n",
    "\n",
    "\n",
    "def BARTMIP_transform(bags, medoids):#OBS CHECK VALIDITY\n",
    "    \"\"\"Transform bags using BARTMIP\"\"\"\n",
    "    transformed = []\n",
    "    for bag in bags:\n",
    "        dist_features = []\n",
    "        for medoid in medoids:\n",
    "            # Multiple distance measures\n",
    "            min_dist = np.min(cdist(bag, medoid, 'euclidean'))\n",
    "            avg_dist = np.mean(np.min(cdist(bag, medoid, 'euclidean'), axis=1))\n",
    "            hausdorff = average_hausdorff_distance(bag, medoid)\n",
    "            dist_features.extend([min_dist, avg_dist, hausdorff])\n",
    "        transformed.append(dist_features)\n",
    "\n",
    "\n",
    "    return np.array(transformed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def average_hausdorff_distance(bag1, bag2):\n",
    "    \"\"\"Calculate average Hausdorff distance between two 2D arrays\"\"\"\n",
    "    bag1 = bag1[~np.isnan(bag1).any(axis=1)]\n",
    "    bag2 = bag2[~np.isnan(bag2).any(axis=1)]\n",
    "    bag1 = bag1[np.isfinite(bag1).all(axis=1)]\n",
    "    bag2 = bag2[np.isfinite(bag2).all(axis=1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dist_matrix = cdist(bag1, bag2, metric='euclidean')\n",
    "    min_dists_a = [min(euclidean(a, b) for b in bag2) for a in bag1]\n",
    "    min_dists_b = [min(euclidean(b, a) for a in bag1) for b in bag2]\n",
    "    #if np.isnan((avg_dist1 + avg_dist2) / 2):\n",
    "        #print(avg_dist1,avg_dist2)\n",
    "    return (np.mean(min_dists_a) + np.mean(min_dists_b)) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53900df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaling_param(x_train):\n",
    "    \"\"\"Set Scaling paramers \"\"\"\n",
    " \n",
    "        \n",
    "    mean = np.mean(x_train, axis=0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    std[std == 0] = 1 \n",
    "    \n",
    "    x_standardized = (x_train - mean) / std\n",
    "    \n",
    "    min_val = np.min(x_standardized, axis=0)\n",
    "    max_val = np.max(x_standardized, axis=0)\n",
    "    range_val = max_val - min_val\n",
    "    range_val[range_val == 0] = 1  \n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'min': min_val,\n",
    "        'range': range_val\n",
    "    }\n",
    "\n",
    "\n",
    "def apply_scaling(x, scaling_param):\n",
    "    \"\"\"\n",
    "    Flatten and scales data\"\"\"\n",
    "    original_shape = x.shape\n",
    "    if len(x.shape) > 2:\n",
    "        x_flat = x.reshape(-1, x.shape[-1])\n",
    "    else:\n",
    "        x_flat = x\n",
    "    \n",
    "    x_scaled = (x_flat - scaling_param['mean']) / scaling_param['std']\n",
    "    \n",
    "    x_scaled = (x_scaled - scaling_param['min']) / scaling_param['range']\n",
    "    \n",
    "    # Reshape back to original shape if needed\n",
    "    if len(original_shape) > 2:\n",
    "        x_scaled = x_scaled.reshape(original_shape)\n",
    "    \n",
    "    return x_scaled\n",
    "\n",
    "\n",
    "def cross(x, y, n_clusters=10):\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    silhouette_scores = []\n",
    "    connectivity_scores = []\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    counter=0\n",
    "    for train_idx, test_idx in cv.split(x):\n",
    "        counter += 1\n",
    "        print(f\"Processing fold {counter}/5\")\n",
    "        x_train, x_test = x[train_idx], x[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "\n",
    "        # Flatten the data for scaling\n",
    "        x_train_flat = x_train.reshape(-1, x_train.shape[-1])\n",
    "        \n",
    "        # Get scaling values from training data\n",
    "        scaling_values = get_scaling_param(x_train_flat)\n",
    "        \n",
    "        # Apply scaling to both training and test data\n",
    "        x_train_scaled = apply_scaling(x_train, scaling_values)\n",
    "        x_test_scaled = apply_scaling(x_test, scaling_values)\n",
    "\n",
    "        # Compute distance matrix\n",
    "        distance_matrix = compute_distance_matrix(x_train_scaled)\n",
    "        \n",
    "        # Perform clustering\n",
    "        medoids, sil_score, connectivity = clustering(x_train_scaled, distance_matrix, n_clusters)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        connectivity_scores.append(connectivity)\n",
    "        \n",
    "        # Transform data using medoids\n",
    "        x_train_transformed = BARTMIP_transform(x_train_scaled, medoids)\n",
    "        x_test_transformed = BARTMIP_transform(x_test_scaled, medoids)\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=3,\n",
    "            loss_function='RMSE',\n",
    "            verbose=False\n",
    "        )\n",
    "        model.fit(\n",
    "            x_train_transformed, y_train,\n",
    "            eval_set=(x_test_transformed, y_test),\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "        y_pred = model.predict(x_test_transformed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        print(r2_score(y_test, y_pred))\n",
    "    \n",
    "    # Return mean metrics with cluster info\n",
    "    return {\n",
    "        'n_clusters': n_clusters,\n",
    "        'mean_mae': np.mean(mae_scores),\n",
    "        'mean_r2': np.mean(r2_scores),\n",
    "        'mean_silhouette': np.mean(silhouette_scores),\n",
    "        'mean_connectivity': np.mean(connectivity_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451797b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loso(x, y, n_clusters=10):\n",
    "    mae_scores = []\n",
    "    silhouette_scores = []\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    connectivity_scores = []\n",
    "    \n",
    "    cv = LeaveOneOut()\n",
    "    counter= 0\n",
    "    for train_idx, test_idx in cv.split(x):\n",
    "        counter += 1\n",
    "        print(f\"Processing fold {counter}/80\")\n",
    "        x_train, x_test = x[train_idx], x[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # Get scaling values from training data\n",
    "        scaling_values = get_scaling_param(x_train.reshape(-1, x_train.shape[-1]))\n",
    "        \n",
    "        # Apply scaling to both training and test data\n",
    "        x_train = apply_scaling(x_train, scaling_values)\n",
    "        x_test = apply_scaling(x_test, scaling_values)\n",
    "        \n",
    "        # Compute distance matrix \n",
    "        distance_matrix = compute_distance_matrix(x_train)\n",
    "        \n",
    "        # Perform clustering\n",
    "        medoids, sil_score,connectivity = clustering(x_train, distance_matrix, n_clusters)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        connectivity_scores.append(connectivity)\n",
    "        \n",
    "        # Transform data using medoids\n",
    "        x_train_transformed = BARTMIP_transform(x_train, medoids)\n",
    "        x_test_transformed = BARTMIP_transform(x_test, medoids)\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=3,\n",
    "            loss_function='RMSE',\n",
    "            verbose=False\n",
    "        )\n",
    "        model.fit(\n",
    "            x_train_transformed, y_train,\n",
    "            eval_set=(x_test_transformed, y_test),\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "        y_pred = model.predict(x_test_transformed)\n",
    "        \n",
    "        # Collect all predictions and true values\n",
    "        all_y_true.append(y_test[0])\n",
    "        all_y_pred.append(y_pred[0])\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    # Calculate global metrics\n",
    "    global_r2 = r2_score(all_y_true, all_y_pred)\n",
    "    \n",
    "    return {\n",
    "        'n_clusters': n_clusters,\n",
    "        'mean_mae': np.mean(mae_scores),\n",
    "        'mean_r2': global_r2,\n",
    "        'mean_silhouette': np.mean(silhouette_scores),\n",
    "        'mean_connectivity': np.mean(connectivity_scores)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "results.append(loso(weekly_bags, biweekly_scores, n_clusters=13))\n",
    "\n",
    "sorted_results = sorted(results, key=lambda x: x['mean_mae'])\n",
    "\n",
    "# Print formatted results\n",
    "print(\"Rank | Clusters | Mean MAE | Mean R² | Mean Silhouette\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "for idx, result in enumerate(sorted_results):\n",
    "    print(f\"{idx+1:4} | {result['n_clusters']:8} | {result['mean_mae']:.4f} | {result['mean_r2']:.4f} | {result['mean_silhouette']:.4f} | {result['mean_connectivity']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
