{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import skew, spearmanr\n",
    "import shap\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids \n",
    "import statistics\n",
    "\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "seed = 69\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40c79e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('all-features-imputed-v2.csv')\n",
    "\n",
    "#display(data)\n",
    "#display(data.columns.to_list())\n",
    "\n",
    "\n",
    "motion         = data.filter(like='motion').columns.tolist()\n",
    "motion        += data.filter(like='acceleration').columns.tolist()\n",
    "motion        += data.filter(like='step').columns.tolist()\n",
    "motion        += data.filter(like='position').columns.tolist()\n",
    "\n",
    "\n",
    "heartrate = data.filter(like='heartrate').filter(regex='^(?!.*sleep)').columns.tolist() #+ [col for col in ['age', 'sex'] if col in data.columns]\n",
    "sleep           = data.filter(like='sleep').columns.tolist()\n",
    "demographics    = ['age','sex']\n",
    "\n",
    "\n",
    "\n",
    "#display(heartrate)\n",
    "#display(motion)\n",
    "#display(sleep)\n",
    "\n",
    "modalities = motion +heartrate + sleep + demographics\n",
    "\n",
    "sensor = data[modalities]\n",
    "\n",
    "x= np.array(sensor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8ee52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_sensor = data[motion]\n",
    "heartrate_sensor = data[heartrate]\n",
    "sleep_sensor = data[sleep]\n",
    "demographic_array = data[demographics].to_numpy()\n",
    "\n",
    "\n",
    "#Set Clinical Score here\n",
    "target = data['sis']\n",
    "participant = data['participant']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "motion_sensor = np.array(motion_sensor)\n",
    "heartrate_sensor=np.array(heartrate_sensor)\n",
    "sleep_sensor =np.array(sleep_sensor)\n",
    "y = np.array(target)\n",
    "p = np.array(participant)\n",
    "\n",
    "\n",
    "#feature_names = sensor.columns.to_list()\n",
    "\n",
    "#display(x.shape, y.shape, p.shape, len(feature_names), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38574f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TRUES = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "SHAP = []\n",
    "X_TEST = []\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "    \n",
    "    print(f\"Fold {fold}: train={len(train_idx)} test={len(test_idx)}\")\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    x_train = normalizer.fit_transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=3,\n",
    "        loss_function='RMSE',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, y_train, eval_set=(x_train, y_train), use_best_model=True, early_stopping_rounds=100)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "    SHAP.append(shap_values)\n",
    "    X_TEST.append(x_test)\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': modalities,\n",
    "    'mean_abs_shap': np.abs(np.vstack(SHAP)).mean(axis=0).round(4)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "motion_shap = shap_df[shap_df['feature'].isin(motion)]\n",
    "heartrate_shap = shap_df[shap_df['feature'].isin(heartrate)]\n",
    "sleep_shap = shap_df[shap_df['feature'].isin(sleep)]\n",
    "\n",
    "# Calculate how many features to take from each group\n",
    "total_features = 16\n",
    "n_motion = min(len(motion_shap), max(1, round(total_features * len(motion)/len(shap_df))))\n",
    "n_heartrate = min(len(heartrate_shap), max(1, round(total_features * len(heartrate)/len(shap_df))))\n",
    "n_sleep = min(len(sleep_shap), max(1, round(total_features * len(sleep)/len(shap_df))))\n",
    "\n",
    "# Adjust counts to ensure total is 16\n",
    "while n_motion + n_heartrate + n_sleep > total_features:\n",
    "    # Reduce the largest group\n",
    "    if n_motion >= n_heartrate and n_motion >= n_sleep:\n",
    "        n_motion -= 1\n",
    "    elif n_heartrate >= n_motion and n_heartrate >= n_sleep:\n",
    "        n_heartrate -= 1\n",
    "    else:\n",
    "        n_sleep -= 1\n",
    "\n",
    "# Select top features from each group\n",
    "top_motion = motion_shap.head(n_motion)['feature'].tolist()\n",
    "top_heartrate = heartrate_shap.head(n_heartrate)['feature'].tolist()\n",
    "top_sleep = sleep_shap.head(n_sleep)['feature'].tolist()\n",
    "\n",
    "\n",
    "print(top_motion)\n",
    "print(top_sleep)\n",
    "print(top_heartrate)\n",
    "\n",
    "x_motion = np.array(data[top_motion])\n",
    "x_heartrate = np.array(data[top_heartrate])\n",
    "x_sleep = np.array(data[top_sleep])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ce51438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the target variable to match weekly scores\n",
    "weekly_scores = y.reshape(-1, 7)[:, 0]  # Shape: (80 weeks, 7 day)\n",
    "\n",
    "#Reshaping the sensor data to match weekly scores\n",
    "motion_bags = motion_sensor.reshape((80, 7, -1))\n",
    "heartrate_bags = heartrate_sensor.reshape((80, 7, -1))\n",
    "#demographic_bags= demographic_array.reshape((80, 7, -1))  \n",
    "sleep_bags = sleep_sensor.reshape((80, 7, -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "492c5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_connectivity(distance_matrix, cluster_labels, k_neighbors=5):\n",
    "    \"\"\"\n",
    "    Compute connectivity score for clustering results.\n",
    "    Lower values indicate better connectivity.\n",
    "    \"\"\"\n",
    "    n_samples = distance_matrix.shape[0]\n",
    "    \n",
    "    # Get k nearest neighbors for each point (excluding self)\n",
    "    neighbor_indices = np.argsort(distance_matrix, axis=1)[:, 1:k_neighbors+1]  # Exclude self\n",
    "    \n",
    "    connectivity = 0\n",
    "    for i in range(n_samples):\n",
    "        for neighbor in neighbor_indices[i]:\n",
    "            if cluster_labels[i] != cluster_labels[neighbor]:\n",
    "                connectivity += 1\n",
    "                \n",
    "    # Normalize to [0,1] range\n",
    "    max_possible = n_samples * k_neighbors\n",
    "    return connectivity / max_possible if max_possible > 0 else 0\n",
    "\n",
    "\n",
    "def compute_distance_matrix(bags):\n",
    "    \"\"\"Compute the distance matrix for a list of bags using average Hausdorff distance.\"\"\"\n",
    "    n= len(bags)\n",
    "    matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dist = average_hausdorff_distance(bags[i], bags[j])\n",
    "            matrix[i, j] = matrix[j, i] = dist\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def clustering(bags,matrix, n_clusters=10):\n",
    "    \"\"\"Perform Clustering using K-medoids\"\"\"\n",
    "    kmedoids=KMedoids(n_clusters=n_clusters, metric='precomputed', method='pam', init=\"build\")\n",
    "                \n",
    "    clusters = kmedoids.fit_predict(matrix)\n",
    "    silhouette_avg = silhouette_score(matrix, clusters, metric=\"precomputed\")\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "    valid_medoids = [idx for idx in kmedoids.medoid_indices_ if idx < len(bags)]\n",
    "    medoids = [bags[idx] for idx in valid_medoids]\n",
    "\n",
    "    return medoids,silhouette_avg, clusters\n",
    "\n",
    "\n",
    "\n",
    "def average_hausdorff_distance(bag1, bag2):\n",
    "    \"\"\"Calculate average Hausdorff distance between two 2D arrays\"\"\"\n",
    "    bag1 = bag1[~np.isnan(bag1).any(axis=1)]\n",
    "    bag2 = bag2[~np.isnan(bag2).any(axis=1)]\n",
    "    bag1 = bag1[np.isfinite(bag1).all(axis=1)]\n",
    "    bag2 = bag2[np.isfinite(bag2).all(axis=1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    dist_matrix = cdist(bag1, bag2, metric='euclidean')\n",
    "    min_dists_a = [min(euclidean(a, b) for b in bag2) for a in bag1]\n",
    "    min_dists_b = [min(euclidean(b, a) for a in bag1) for b in bag2]\n",
    "   \n",
    "    return (np.mean(min_dists_a) + np.mean(min_dists_b)) / 2\n",
    "\n",
    "\n",
    "def clustering(bags, distance_matrix, n_clusters=10):\n",
    "    \"\"\"Perform clustering and return metrics\"\"\"\n",
    "    kmedoids = KMedoids(n_clusters=n_clusters, \n",
    "                       metric='precomputed', \n",
    "                       method='pam',\n",
    "                       init=\"build\")\n",
    "    \n",
    "    clusters = kmedoids.fit_predict(distance_matrix)\n",
    "    silhouette_avg = silhouette_score(distance_matrix, clusters, metric=\"precomputed\")\n",
    "    connectivity = compute_connectivity(distance_matrix, clusters)\n",
    "    \n",
    "    # Handle medoid validation\n",
    "    valid_medoids = [idx for idx in kmedoids.medoid_indices_ if idx < len(bags)]\n",
    "    medoids = [bags[idx] for idx in valid_medoids]\n",
    "    \n",
    "    return medoids, silhouette_avg, connectivity, clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scaling_param(training_bags):\n",
    "            \"\"\"Flattens data for scaling and computes mean, std, min, max.\"\"\"\n",
    "            flat_data = np.concatenate([arr.reshape(-1, arr.shape[-1]) for arr in training_bags], axis=0)\n",
    "            return {\n",
    "                'mean': np.mean(flat_data, axis=0),\n",
    "                'std': np.std(flat_data, axis=0),\n",
    "                'min': np.min(flat_data, axis=0),\n",
    "                'max': np.max(flat_data, axis=0)\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e863d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_features(cluster_assignments, y, n_clusters):\n",
    "    \"\"\"Compute features for each cluster based on regression labels\"\"\"\n",
    "    features = []\n",
    "    for j in range(n_clusters):\n",
    "        mask = (cluster_assignments == j)\n",
    "        labels = y[mask]\n",
    "        if len(labels) == 0:\n",
    "            features.append([np.nan]*5)\n",
    "            continue\n",
    "        min_val = np.min(labels)\n",
    "        max_val = np.max(labels)\n",
    "        mean_val = np.mean(labels)\n",
    "        std_val = np.std(labels)\n",
    "        median_val = np.median(labels)\n",
    "        features.append([min_val, max_val, mean_val, std_val, median_val])\n",
    "    return np.array(features)\n",
    "def transform_bag(training_bag, consensus_medoids):\n",
    "    #training_bag is a dict: {'motion': motion_bag, 'heartrate': hr_bag, 'sleep': sleep_bag}\n",
    "    features = []\n",
    "    for cluster_idx in range(len(consensus_medoids)):\n",
    "        medoids = consensus_medoids[cluster_idx]\n",
    "        distances = []\n",
    "        for (view, medoid_bag) in medoids:\n",
    "            bag = training_bag[view]\n",
    "            dist = average_hausdorff_distance(bag, medoid_bag)\n",
    "            distances.append(dist)\n",
    "        avg_dist = np.mean(distances) if distances else 0\n",
    "        features.append(avg_dist)\n",
    "    return np.array(features)\n",
    "def perform_consensus_clustering(train_bags, y_train):\n",
    "    \"\"\"Performs multi-view consensus clustering with set cluster counts\"\"\"\n",
    "    # 1. Cluster each view with optimal k\n",
    "    best_k = {}\n",
    "    best_clusters = {}\n",
    "    best_medoids = {}\n",
    "    sil_list = []\n",
    "    con_list = []\n",
    "\n",
    "    \n",
    "    for view in ['motion', 'heartrate', 'sleep']:#, 'demographics']:\n",
    "        bags = train_bags[view]\n",
    "        \n",
    "        \n",
    "        #Cluster count for each view\n",
    "        \n",
    "        distance_matrix = compute_distance_matrix(bags)\n",
    "        if view==\"motion\":\n",
    "            n_clusters =6\n",
    "        if view==\"heartrate\":\n",
    "            n_clusters = 6\n",
    "        if view==\"sleep\":\n",
    "            n_clusters = 9\n",
    "        if view==\"demographics\":\n",
    "            n_clusters = 6\n",
    "        \n",
    "        medoids, silhouette_avg, connectivity, clusters = clustering(bags, distance_matrix, n_clusters)\n",
    "        \n",
    "        best_n = n_clusters\n",
    "        best_clusters[view] = clusters\n",
    "        best_medoids[view] = medoids\n",
    "        \n",
    "        sil_list.append(silhouette_avg)\n",
    "        con_list.append(connectivity)\n",
    "        best_k[view] = best_n\n",
    "\n",
    "    #Generate cluster features using regression labels\n",
    "    view_features = {}\n",
    "    for view in ['motion', 'heartrate', 'sleep']:#, 'demographics']:\n",
    "        features = compute_cluster_features(best_clusters[view], y_train, best_k[view])\n",
    "        view_features[view] = features\n",
    "\n",
    "    # Aggregate all features and their origins\n",
    "    all_features = []\n",
    "    cluster_origins = []  # Track (view, cluster_idx)\n",
    "    \n",
    "    for view in ['motion', 'heartrate', 'sleep']:#, 'demographics']:\n",
    "        for idx, feat in enumerate(view_features[view]):\n",
    "                all_features.append(feat)\n",
    "                cluster_origins.append((view, idx))\n",
    "    \n",
    "    all_features = np.array(all_features)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    best_consensus_k = 6\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Consensus clustering using K-medoids\n",
    "    kmedoids_consensus = KMedoids(\n",
    "        n_clusters=best_consensus_k,\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    consensus_labels = kmedoids_consensus.fit_predict(all_features)\n",
    "    sil_score = silhouette_score(all_features, consensus_labels)\n",
    "    #print(sil_score)\n",
    "\n",
    "    consensus_connectivity = 0.0\n",
    "    medoid_indices = kmedoids_consensus.medoid_indices_\n",
    "    for i, label in enumerate(consensus_labels):\n",
    "        medoid_idx = medoid_indices[label]\n",
    "        consensus_connectivity += np.linalg.norm(all_features[i] - all_features[medoid_idx])\n",
    "        consensus_connectivity /= len(all_features)\n",
    "\n",
    "\n",
    "    # Mapping to original medoids\n",
    "    consensus_medoids = {}\n",
    "    for cluster_idx in range(best_consensus_k):\n",
    "        indices = np.where(consensus_labels == cluster_idx)[0]\n",
    "        origins = [cluster_origins[i] for i in indices]\n",
    "        \n",
    "        medoids = []\n",
    "        for (view, orig_cluster) in origins:\n",
    "            # Bounds Check\n",
    "            if orig_cluster < len(best_medoids[view]):\n",
    "                medoid_bag = best_medoids[view][orig_cluster]\n",
    "                medoids.append((view, medoid_bag))\n",
    "        \n",
    "        consensus_medoids[cluster_idx] = medoids\n",
    "\n",
    "\n",
    "    return consensus_medoids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53900df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cross(bags_dict, y, n_folds=5):\n",
    "    \"\"\" Perform cross-validation with consensus clustering and CatBoost regression.\"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    fold_info = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(y)):\n",
    "        print(f\"Processing fold {fold+1}/{n_folds}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        train_bags = {view: [bags_dict[view][i] for i in train_idx] for view in bags_dict}\n",
    "        test_bags = {view: [bags_dict[view][i] for i in test_idx] for view in bags_dict}\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "       \n",
    "\n",
    "        # Set scaling Params \n",
    "        scaling_param = {\n",
    "            view: get_scaling_param(\n",
    "                [item for sublist in [train_bags[view], test_bags[view]] for item in sublist]\n",
    "            )\n",
    "            for view in ['motion', 'heartrate', 'sleep']\n",
    "        }\n",
    "\n",
    "        scale_bag = lambda bag, stats: (\n",
    "            lambda reshaped: (\n",
    "                (reshaped - stats['mean']) / stats['std'] - stats['min']\n",
    "            ) / (stats['max'] - stats['min'])\n",
    "        )(bag.reshape(-1, bag.shape[-1])).reshape(bag.shape)\n",
    "\n",
    "        # Scaling of bags\n",
    "        for view in ['motion', 'heartrate', 'sleep']:#, 'demographics']:\n",
    "            train_bags[view] = [scale_bag(bag, scaling_param[view]) for bag in train_bags[view]]\n",
    "            test_bags[view] = [scale_bag(bag, scaling_param[view]) for bag in test_bags[view]]\n",
    "\n",
    "        # Set up Consensus medoids using training bags\n",
    "        consensus_medoids = perform_consensus_clustering(train_bags, y_train)\n",
    "        \n",
    "        # Transform bags using consensus medoids derived from clustering\n",
    "        X_train_transformed = np.array([transform_bag({\n",
    "            'motion': train_bags['motion'][i],\n",
    "            'heartrate': train_bags['heartrate'][i],\n",
    "            'sleep': train_bags['sleep'][i]\n",
    "        }, consensus_medoids) for i in range(len(train_idx))])\n",
    "        \n",
    "        X_test_transformed = np.array([transform_bag({\n",
    "            'motion': test_bags['motion'][i],\n",
    "            'heartrate': test_bags['heartrate'][i],\n",
    "            'sleep': test_bags['sleep'][i]\n",
    "        }, consensus_medoids) for i in range(len(test_idx))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Train and evaluate CatBoost\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=3,\n",
    "            loss_function='RMSE',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_transformed, y_train,\n",
    "            eval_set=(X_test_transformed, y_test),\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_test_transformed)\n",
    "        \n",
    "        # Store metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        r2_scores.append(r2)\n",
    "        \n",
    "        fold_info.append({\n",
    "            'fold': fold+1,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'n_train': len(train_idx),\n",
    "            'n_test': len(test_idx)\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'global_mae': np.mean(mae_scores),\n",
    "        'global_r2': np.mean(r2_scores),\n",
    "        'fold_details': fold_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c19a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loso(bags_dict, y):\n",
    "    \"\"\" Perform Leave-One-Sample-Out cross-validation with consensus clustering and CatBoost regression.\"\"\"\n",
    "    n_samples = len(y)\n",
    "    loo = LeaveOneOut()\n",
    "    abs_errors = []  \n",
    "    all_y_true = []  \n",
    "    all_y_pred = []  \n",
    "    fold_info = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(loo.split(y)):\n",
    "        print(f\"Processing fold {fold+1}/{n_samples}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        train_bags = {view: [bags_dict[view][i] for i in train_idx] for view in bags_dict}\n",
    "        test_bags = {view: [bags_dict[view][i] for i in test_idx] for view in bags_dict}\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        true_val = y_test[0]  # Extract scalar value\n",
    "    \n",
    "\n",
    "        # Set scaling Params\n",
    "        scaling_param = {\n",
    "            view: get_scaling_param(\n",
    "                [item for sublist in [train_bags[view], test_bags[view]] for item in sublist]\n",
    "            )\n",
    "            for view in ['motion', 'heartrate', 'sleep']#, 'demographics']\n",
    "        }\n",
    "\n",
    "        scale_bag = lambda bag, stats: (\n",
    "            lambda reshaped: (\n",
    "                (reshaped - stats['mean']) / stats['std'] - stats['min']\n",
    "            ) / (stats['max'] - stats['min'])\n",
    "        )(bag.reshape(-1, bag.shape[-1])).reshape(bag.shape)\n",
    "\n",
    "        # Scale bags\n",
    "        for view in ['motion', 'heartrate', 'sleep']:#, 'demographics']:\n",
    "            train_bags[view] = [scale_bag(bag, scaling_param[view]) for bag in train_bags[view]]\n",
    "            test_bags[view] = [scale_bag(bag, scaling_param[view]) for bag in test_bags[view]]\n",
    "\n",
    "        # 1. Cluster using TRAINING DATA ONLY\n",
    "        consensus_medoids = perform_consensus_clustering(train_bags, y_train)\n",
    "        \n",
    "           \n",
    "        # Transform bags using consensus medoids derived from clustering\n",
    "        X_train_transformed = np.array([transform_bag({\n",
    "            'motion': train_bags['motion'][i],\n",
    "            'heartrate': train_bags['heartrate'][i],\n",
    "            'sleep': train_bags['sleep'][i]\n",
    "            #'demographics': train_bags['demographics'][i]\n",
    "        }, consensus_medoids) for i in range(len(train_idx))])\n",
    "        \n",
    "        X_test_transformed = np.array([transform_bag({\n",
    "            'motion': test_bags['motion'][0],\n",
    "            'heartrate': test_bags['heartrate'][0],\n",
    "            'sleep': test_bags['sleep'][0]\n",
    "            #'demographics': test_bags['demographics'][0]\n",
    "        }, consensus_medoids)])\n",
    "        \n",
    "\n",
    "        # 3. Train and evaluate CatBoost\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=3,\n",
    "            loss_function='RMSE',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_transformed, y_train,\n",
    "            eval_set=(X_test_transformed, y_test),\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "        \n",
    "        pred_val = model.predict(X_test_transformed)[0]  \n",
    "        \n",
    "        # Store metrics for final calculation\n",
    "        abs_error = abs(true_val - pred_val)\n",
    "        abs_errors.append(abs_error)\n",
    "        all_y_true.append(true_val)\n",
    "        all_y_pred.append(pred_val)\n",
    "        \n",
    "        fold_info.append({\n",
    "            'fold': fold+1,\n",
    "            'abs_error': abs_error,\n",
    "            'true': true_val,\n",
    "            'pred': pred_val,\n",
    "            'n_train': len(train_idx),\n",
    "            'n_test': 1\n",
    "        })\n",
    "\n",
    "    global_mae = np.mean(abs_errors)\n",
    "    global_r2 = r2_score(all_y_true, all_y_pred)\n",
    "    \n",
    "    return {\n",
    "        'global_mae': global_mae,\n",
    "        'global_r2': global_r2,\n",
    "        'fold_details': fold_info,\n",
    "        'all_true': all_y_true,\n",
    "        'all_pred': all_y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sil_list = []\n",
    "con_list= []\n",
    "bags_dict = {\n",
    "    'motion': motion_bags,\n",
    "    'heartrate': heartrate_bags,\n",
    "    'sleep': sleep_bags\n",
    "    #'demographics': demographic_bags\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(0,1):\n",
    "    results = loso(bags_dict, weekly_scores)\n",
    "    print(f\"Final MAE: {results['global_mae']:.3f}, R²: {results['global_r2']:.3f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
